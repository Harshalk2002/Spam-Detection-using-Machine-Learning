üìß Spam Detection using Machine Learning


üìö Overview
This project implements a Spam Detection system for classifying SMS messages as Spam or Ham (Not Spam) using Machine Learning techniques.
The models were trained and evaluated on the SMSSpamCollection dataset from the UCI Machine Learning Repository.
The primary goal is to filter out unwanted and fraudulent messages while maintaining a balance between Precision and Recall.

üóÇÔ∏è Table of Contents
Overview
Dataset
Preprocessing
Exploratory Data Analysis
Models Implemented
Results
Challenges & Solutions
Conclusion
How to Run
Requirements

Dataset Name: SMSSpamCollection Dataset
Description:
Contains 5,574 English SMS messages labeled as either:
Spam: Unwanted promotional/fraudulent messages
Ham: Legitimate messages
Source: UCI Machine Learning Repository
üßπ Preprocessing
Converted all text to lowercase
Removed punctuation, numbers, and special characters
Removed extra spaces
Tokenization and TF-IDF Vectorization for feature extraction
Dataset split: 80% training, 20% testing
üìä Exploratory Data Analysis
Class Distribution:
Visualized the ratio of Spam vs. Ham messages
Message Length Distribution:
Spam messages are typically shorter
Common Spam Words:
"free", "win", "text", "claim", etc.
ü§ñ Models Implemented
Model	Description
Na√Øve Bayes	MultinomialNB, ideal for text classification tasks
Logistic Regression	Regularized Logistic Regression, with GridSearchCV for hyperparameter tuning
‚úÖ Results
Na√Øve Bayes:
Accuracy: 96.86%
Precision: 100%
Recall: 76.51%
F1-Score: 86.69%
Logistic Regression:
Accuracy: 96.77%
Precision: 99.13%
Recall: 76.51%
F1-Score: 86.36%
Confusion Matrix Analysis:
Na√Øve Bayes achieved perfect precision, meaning no false positives
Logistic Regression delivered a better balance between precision and accuracy
‚öñÔ∏è Model Comparison
Model	Pros	Cons
Na√Øve Bayes	Perfect Precision (100%)	Lower Recall (76.51%)
Logistic Regression	Balanced performance	Slightly lower Precision
‚û°Ô∏è Best Choice?

Use Na√Øve Bayes if avoiding false positives is critical
Use Logistic Regression for more balanced predictions
Future improvements: Ensemble models, deep learning approaches
üöß Challenges & Solutions
Balancing Precision and Recall
‚û°Ô∏è Used different classifiers and hyperparameter tuning
Preprocessing Optimization
‚û°Ô∏è Switched from Count Vectorizer to TF-IDF for better results
Logistic Regression convergence issues
‚û°Ô∏è Increased max_iter to 500 to resolve warnings
üéØ Conclusion
Both models successfully detect spam messages with high accuracy.

Na√Øve Bayes provides zero false positives
Logistic Regression gives a balanced approach
Future work may explore ensemble methods and deep learning techniques for improved performance.


‚öñÔ∏è License
This project is licensed under the MIT License.

‚≠êÔ∏è Acknowledgments
UCI Machine Learning Repository
scikit-learn and the Python open-source community
Matplotlib and Seaborn for visualization tools

